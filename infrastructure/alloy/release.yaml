---
# HelmRelease for Grafana Alloy
# OpenTelemetry collector and log aggregation agent
#
# Pipelines:
#   1. Logs: Kubernetes API -> Loki
#   2. Traces: OTLP (4317/4318) -> Tempo
#
# Applications send traces to: monitoring-alloy.monitoring.svc.cluster.local:4318 (HTTP)
#                          or: monitoring-alloy.monitoring.svc.cluster.local:4317 (gRPC)
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: alloy
  namespace: flux-system
spec:
  interval: 10m
  chart:
    spec:
      chart: alloy
      version: '>=0.9.0 <1.0.0'
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
      interval: 1h

  targetNamespace: monitoring
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    # DaemonSet on Raspberry Pis needs more time to roll out
    timeout: 15m
    remediation:
      retries: 3

  values:
    # ==================================================================
    # ALLOY CONFIGURATION
    # River config defining the log collection pipeline
    # Uses Kubernetes API to fetch logs (no hostPath needed!)
    # ==================================================================
    alloy:
      configMap:
        content: |
          // ============================================================
          // ALLOY TELEMETRY PIPELINES
          // This configuration defines multiple independent pipelines:
          //   1. Kubernetes logs -> Loki
          //   2. OTLP traces -> Tempo
          //   3. OTLP metrics -> Prometheus (future)
          // ============================================================


          // ============================================================
          // PIPELINE 1: KUBERNETES LOG COLLECTION
          // Uses Kubernetes API to stream pod logs (no hostPath mount needed)
          // This approach works with any PodSecurity policy
          // ============================================================

          // Discover all pods in the cluster
          discovery.kubernetes "pods" {
            role = "pod"
          }

          // Relabel to extract useful labels for Loki
          discovery.relabel "pods" {
            targets = discovery.kubernetes.pods.targets

            // Keep only running pods
            rule {
              source_labels = ["__meta_kubernetes_pod_phase"]
              regex         = "Pending|Succeeded|Failed|Unknown"
              action        = "drop"
            }

            // Set the namespace label
            rule {
              source_labels = ["__meta_kubernetes_namespace"]
              target_label  = "namespace"
            }

            // Set the pod label
            rule {
              source_labels = ["__meta_kubernetes_pod_name"]
              target_label  = "pod"
            }

            // Set the container label
            rule {
              source_labels = ["__meta_kubernetes_pod_container_name"]
              target_label  = "container"
            }

            // Set the node label
            rule {
              source_labels = ["__meta_kubernetes_pod_node_name"]
              target_label  = "node"
            }

            // Set app label from common label patterns
            rule {
              source_labels = ["__meta_kubernetes_pod_label_app"]
              target_label  = "app"
            }
            rule {
              source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
              target_label  = "app"
            }
          }

          // Collect logs via Kubernetes API
          // This streams logs directly from the API server, no file access needed
          loki.source.kubernetes "pods" {
            targets    = discovery.relabel.pods.output
            forward_to = [loki.process.pods.receiver]
          }

          // Process logs before sending to Loki
          loki.process "pods" {
            forward_to = [loki.write.default.receiver]

            // Add static job label
            stage.static_labels {
              values = {
                job = "kubernetes-pods",
              }
            }
          }

          // Ship logs to Loki
          // Service name is "monitoring-loki" (helm release name prefix)
          loki.write "default" {
            endpoint {
              url = "http://monitoring-loki.monitoring.svc.cluster.local:3100/loki/api/v1/push"
            }
          }


          // ============================================================
          // PIPELINE 2: OPENTELEMETRY TRACES
          // Receives traces via OTLP HTTP/gRPC and forwards to Tempo
          //
          // Data flow:
          //   App --OTLP--> otelcol.receiver.otlp
          //                        |
          //                        v
          //                 otelcol.processor.batch (batches for efficiency)
          //                        |
          //                        v
          //                 otelcol.exporter.otlp --> Tempo
          // ============================================================

          // OTLP Receiver - accepts traces from instrumented applications
          // Listens on both gRPC (4317) and HTTP (4318) protocols
          // Applications send traces here using the OTLP protocol
          otelcol.receiver.otlp "default" {
            // gRPC endpoint - more efficient, used by most SDKs by default
            grpc {
              endpoint = "0.0.0.0:4317"
            }
            // HTTP endpoint - easier to debug with curl, better proxy support
            http {
              endpoint = "0.0.0.0:4318"
            }

            // Forward received telemetry to the batch processor
            output {
              traces  = [otelcol.processor.batch.default.input]
              // Future: metrics and logs can be added here
              // metrics = [otelcol.processor.batch.default.input]
              // logs    = [otelcol.processor.batch.default.input]
            }
          }

          // Batch Processor - groups traces before sending to reduce overhead
          // Instead of sending each span immediately, we batch them together
          // This reduces network calls and improves throughput significantly
          otelcol.processor.batch "default" {
            // Send a batch when we hit 8KB of data
            send_batch_size = 8192
            // Or after 200ms, whichever comes first
            timeout = "200ms"

            output {
              traces  = [otelcol.exporter.otlp.tempo.input]
            }
          }

          // OTLP Exporter - sends traces to Tempo
          // Tempo accepts OTLP natively, so we just forward the traces
          otelcol.exporter.otlp "tempo" {
            // Tempo's OTLP gRPC endpoint
            client {
              endpoint = "tempo.monitoring.svc.cluster.local:4317"
              // TLS disabled for in-cluster communication
              tls {
                insecure = true
              }
            }
          }

      # No volume mounts needed - we use the Kubernetes API!
      mounts:
        varlog: false

      # Clustering ensures logs aren't duplicated across replicas
      clustering:
        enabled: true

    # ==================================================================
    # DEPLOYMENT CONFIGURATION
    # Using Deployment instead of DaemonSet since we're reading from API
    # ==================================================================
    controller:
      type: deployment
      replicas: 2

    # ==================================================================
    # RESOURCE LIMITS
    # Keep it light for Raspberry Pi (bumped slightly for OTel processing)
    # ==================================================================
    resources:
      limits:
        cpu: 300m
        memory: 384Mi
      requests:
        cpu: 100m
        memory: 192Mi

    # ==================================================================
    # SERVICE CONFIGURATION
    # Expose OTLP ports for trace ingestion from applications
    # ==================================================================
    alloy:
      extraPorts:
        # OTLP gRPC receiver - standard OpenTelemetry port
        - name: otlp-grpc
          port: 4317
          targetPort: 4317
          protocol: TCP
        # OTLP HTTP receiver - easier to debug, works with curl
        - name: otlp-http
          port: 4318
          targetPort: 4318
          protocol: TCP
